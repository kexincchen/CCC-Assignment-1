{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiest hour: 2.0\n",
      "Happiest day: 2021-06-21\n",
      "Most active hour: 1.0\n",
      "Most active day: 2021-06-21\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "with open('data/twitter-50mb.json') as f:\n",
    "    data = json.load(f)\n",
    "    tweets_df = pd.json_normalize(data['rows'])\n",
    "\n",
    "    # tweets_df = pd.DataFrame()\n",
    "    # tweets_df['created_at'] = pd.to_datetime(tweets_df['doc.data.created_at'])\n",
    "    tweets_df['doc.data.created_at'] = pd.to_datetime(tweets_df['doc.data.created_at'])\n",
    "    # tweets_df['sentiment_score'] = tweets_df['doc.data.sentiment'] \n",
    "\n",
    "    tweets_df['hour'] = tweets_df['doc.data.created_at'].dt.hour\n",
    "    tweets_df['day'] = tweets_df['doc.data.created_at'].dt.date\n",
    "\n",
    "    happiest_hour = tweets_df.groupby('hour')['doc.data.sentiment'].sum().idxmax()\n",
    "    happiest_day = tweets_df.groupby('day')['doc.data.sentiment'].sum().idxmax()\n",
    "    most_active_hour = tweets_df['hour'].value_counts().idxmax()\n",
    "    most_active_day = tweets_df['day'].value_counts().idxmax()\n",
    "\n",
    "\n",
    "    print(f\"Happiest hour: {happiest_hour}\")\n",
    "    print(f\"Happiest day: {happiest_day}\")\n",
    "    print(f\"Most active hour: {most_active_hour}\")\n",
    "    print(f\"Most active day: {most_active_day}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiest hour: 2.0\n",
      "Happiest day: 2021-06-21\n",
      "Most active hour: 1.0\n",
      "Most active day: 2021-06-21\n"
     ]
    }
   ],
   "source": [
    "df = pd.json_normalize(data['rows'])\n",
    "\n",
    "tweets_df = pd.DataFrame()\n",
    "tweets_df['created_at'] = pd.to_datetime(df['doc.data.created_at'])\n",
    "tweets_df['sentiment_score'] = df['doc.data.sentiment'] \n",
    "\n",
    "tweets_df['hour'] = tweets_df['created_at'].dt.hour\n",
    "tweets_df['day'] = tweets_df['created_at'].dt.date\n",
    "\n",
    "happiest_hour = tweets_df.groupby('hour')['sentiment_score'].sum().idxmax()\n",
    "happiest_day = tweets_df.groupby('day')['sentiment_score'].sum().idxmax()\n",
    "most_active_hour = tweets_df['hour'].value_counts().idxmax()\n",
    "most_active_day = tweets_df['day'].value_counts().idxmax()\n",
    "\n",
    "\n",
    "print(f\"Happiest hour: {happiest_hour}\")\n",
    "print(f\"Happiest day: {happiest_day}\")\n",
    "print(f\"Most active hour: {most_active_hour}\")\n",
    "print(f\"Most active day: {most_active_day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiest hour: 1\n",
      "Happiest day: 2021-06-21\n",
      "Most active hour: 1\n",
      "Most active day: 2021-06-21\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "for tweet in data['rows']:\n",
    "    # if 'value' not in tweet:\n",
    "    #     continue\n",
    "    if 'doc' not in tweet:\n",
    "        continue\n",
    "    if 'sentiment' not in tweet['doc']['data']:\n",
    "        continue\n",
    "    # created_at = datetime.fromisoformat(tweet['doc']['data']['created_at'])\n",
    "    created_at = parser.parse(tweet['doc']['data']['created_at'])\n",
    "    sentiment_score = tweet['doc']['data']['sentiment']\n",
    "    if isinstance(sentiment_score, dict):\n",
    "        sentiment_score = sentiment_score['score']\n",
    "    # print(sentiment_score)\n",
    "    tweets.append({'created_at': created_at, 'sentiment_score': sentiment_score})\n",
    "\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "tweets_df['created_at'] = pd.to_datetime(df['doc.data.created_at'])\n",
    "tweets_df['sentiment_score'] = df['doc.data.sentiment'] \n",
    "\n",
    "tweets_df['hour'] = tweets_df['created_at'].dt.hour\n",
    "tweets_df['day'] = tweets_df['created_at'].dt.date\n",
    "\n",
    "happiest_hour = tweets_df.groupby('hour')['sentiment_score'].sum().idxmax()\n",
    "happiest_day = tweets_df.groupby('day')['sentiment_score'].sum().idxmax()\n",
    "most_active_hour = tweets_df['hour'].value_counts().idxmax()\n",
    "most_active_day = tweets_df['day'].value_counts().idxmax()\n",
    "\n",
    "\n",
    "print(f\"Happiest hour: {happiest_hour}\")\n",
    "print(f\"Happiest day: {happiest_day}\")\n",
    "print(f\"Most active hour: {most_active_hour}\")\n",
    "print(f\"Most active day: {most_active_day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An error occurred while calling the read_json method registered to the pandas backend.\nOriginal Message: Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/backends.py:135\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/dataframe/io/json.py:300\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(url_path, orient, lines, storage_options, blocksize, sample, encoding, errors, compression, meta, engine, include_path_column, path_converter, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     parts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    287\u001b[0m         delayed(read_json_file)(\n\u001b[1;32m    288\u001b[0m             f,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files\n\u001b[1;32m    298\u001b[0m     ]\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_delayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/dataframe/io/io.py:625\u001b[0m, in \u001b[0;36mfrom_delayed\u001b[0;34m(dfs, meta, divisions, prefix, verify_meta)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_meta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mThis turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    223\u001b[0m task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/dataframe/io/json.py:320\u001b[0m, in \u001b[0;36mread_json_file\u001b[0;34m(f, orient, lines, engine, column_name, path, path_dtype, kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m f \u001b[38;5;28;01mas\u001b[39;00m open_file:\n\u001b[0;32m--> 320\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopen_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column_name:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/json/_json.py:784\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/json/_json.py:973\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    972\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 973\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/json/_json.py:1001\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1001\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/json/_json.py:1134\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/json/_json.py:1347\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1347\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read the JSON data in chunks\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tweets_ddf \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/twitter-50mb.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# # Process the data in parallel\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# tweets_ddf['created_at'] = dd.to_datetime(tweets_ddf['value.doc.data.created_at'])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# tweets_ddf['sentiment_score'] = tweets_ddf['value.doc.data.sentiment']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(f\"Most active hour: {most_active_hour}\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(f\"Most active day: {most_active_day}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/dask/backends.py:137\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: An error occurred while calling the read_json method registered to the pandas backend.\nOriginal Message: Expected object or value"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the JSON data in chunks\n",
    "tweets_ddf = dd.read_json('data/twitter-50mb.json')\n",
    "\n",
    "# # Process the data in parallel\n",
    "# tweets_ddf['created_at'] = dd.to_datetime(tweets_ddf['value.doc.data.created_at'])\n",
    "# tweets_ddf['sentiment_score'] = tweets_ddf['value.doc.data.sentiment']\n",
    "# tweets_ddf['hour'] = tweets_ddf['created_at'].dt.hour\n",
    "# tweets_ddf['day'] = tweets_ddf['created_at'].dt.date\n",
    "\n",
    "# # Calculate the required metrics in parallel\n",
    "# happiest_hour = tweets_ddf.groupby('hour')['sentiment_score'].mean().nlargest(1).idxmax().compute()\n",
    "# happiest_day = tweets_ddf.groupby('day')['sentiment_score'].mean().nlargest(1).idxmax().compute()\n",
    "# most_active_hour = tweets_ddf['hour'].value_counts().idxmax().compute()\n",
    "# most_active_day = tweets_ddf['day'].value_counts().idxmax().compute()\n",
    "\n",
    "# print(f\"Happiest hour: {happiest_hour}\")\n",
    "# print(f\"Happiest day: {happiest_day}\")\n",
    "# print(f\"Most active hour: {most_active_hour}\")\n",
    "# print(f\"Most active day: {most_active_day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rows</th>\n",
       "      <th>offset</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406813874750300166', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406814402272137216', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406920780328312832', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406929052724654084', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406932228165099530', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406801815958528009', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406800773623672833', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406807450959843328', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': '1406810512587771905', 'key': [2021, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>185875709</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_rows  offset                                               rows\n",
       "0       185875709       0  {'id': '1406813874750300166', 'key': [2021, 6,...\n",
       "1       185875709       0  {'id': '1406814402272137216', 'key': [2021, 6,...\n",
       "2       185875709       0  {'id': '1406920780328312832', 'key': [2021, 6,...\n",
       "3       185875709       0  {'id': '1406929052724654084', 'key': [2021, 6,...\n",
       "4       185875709       0  {'id': '1406932228165099530', 'key': [2021, 6,...\n",
       "...           ...     ...                                                ...\n",
       "49995   185875709       0  {'id': '1406801815958528009', 'key': [2021, 6,...\n",
       "49996   185875709       0  {'id': '1406800773623672833', 'key': [2021, 6,...\n",
       "49997   185875709       0  {'id': '1406807450959843328', 'key': [2021, 6,...\n",
       "49998   185875709       0  {'id': '1406810512587771905', 'key': [2021, 6,...\n",
       "49999   185875709       0                                                 {}\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('data/twitter-50mb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.7142857142857143', '-0.09523809523809523']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '{\"text\": \"@pup_chazable Such a cute pupper 😍\",\\\n",
    "          \"sentiment\": 0.7142857142857143\\\n",
    "        },\\\n",
    "        {\"sentiment\":-0.09523809523809523, \"text\": \"yes\"}'\n",
    "pattern = r'\"sentiment\":\\s*(-?\\d+\\.\\d+)'\n",
    "\n",
    "match = re.findall(pattern, text)\n",
    "print(match)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(131, 152), match='\"created_at\": \"2013-\"'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "DATE_PATTERN = re.compile(r'\"created_at\":\\s*\"([^\"]+)\"')\n",
    "SENTIMENT_PATTERN = re.compile(r'\"sentiment\":\\s*(-?\\d+\\.\\d+)')\n",
    "\n",
    "line = '{\"text\": \"@pup_chazable Such a cute pupper 😍\",\\\n",
    "          \"sentiment\": {\"score\": 0.7142857142857143\\\n",
    "        },\\\n",
    "        {\"text\": \"yes\", \"created_at\": \"2013-\"}'\n",
    "        \n",
    "created_at_match = DATE_PATTERN.search(line)\n",
    "sentiment_match = SENTIMENT_PATTERN.search(line)\n",
    "\n",
    "if created_at_match:\n",
    "  print(created_at_match)\n",
    "print(sentiment_match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
